{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7HLdN6RHIoB"
   },
   "source": [
    "## This notebook contains a **tutorial** followed by **Problem 3 of Homework 3**, both of which are based on PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2biosIind-6h"
   },
   "source": [
    "### To complete this problem, please upload this .ipynb (Notebook) file **to Google Colab.** (You'll need to use Colab because it supports GPU and has all the necessary packages pre-installed.) When you're finished, go to \"File -> Download .ipynb\" and submit your .ipynb notebook as a supplemental file.\n",
    "\n",
    "### This notebook has instructions for several homework problems.  ***You must write your answers to these problems in your PDF submission file.*** Your Colab notebook will not be graded, so please make sure you transfer all deliverables (including plots and written work) to your PDF.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "LTOxErZWluNP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Op_QDEYGhCqL"
   },
   "source": [
    "So far in CS 181, we've been using numpy and scikit-learn to write ML code. In this tutorial, we'll introduce you to **PyTorch**, the ML library of choice for industry and academia.\n",
    "\n",
    "**Before working through this notebook, make sure that the runtime type is set to GPU.** Follow Edit -> Notebook settings -> and select GPU as the hardware accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K99RHUFel7yJ"
   },
   "source": [
    "# Tutorial: Getting to Know PyTorch - Key Concepts\n",
    "\n",
    "### Why PyTorch?\n",
    "\n",
    "PyTorch...\n",
    "*   Provides flexible building blocks for building customizable models, including neural network layers, activation functions, and optimization algorithms\n",
    "*   Can automatically compute gradients, making backpropagation a breeze!\n",
    "*   Can accelerate computation on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OqXiibkjd8b"
   },
   "source": [
    "## Tensors\n",
    "\n",
    "Tensors are the basic units of storage and computation in PyTorch. They are very similar to NumPy arrays, and follow the same syntax for indexing and for most arithmetic operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ6v6uTCjcqm",
    "outputId": "d6072439-989c-44db-a57f-44f07f45a0de"
   },
   "outputs": [],
   "source": [
    "# Basic tensor operations\n",
    "x_tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "y_tensor = torch.ones((2, 5))\n",
    "prod = x_tensor * y_tensor[1,:]\n",
    "print(prod, prod.shape) # Use .shape to view shape just as in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "t1POkPN7oHDl"
   },
   "outputs": [],
   "source": [
    "# Easy conversion between PyTorch and Numpy\n",
    "x_numpy = np.array([1, 2, 3, 4, 5])\n",
    "assert (x_tensor.numpy() == x_numpy).all()\n",
    "assert (x_tensor == torch.from_numpy(x_numpy)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0ovXbiFmMA_"
   },
   "source": [
    "What makes tensors **different** from NumPy arrays is:\n",
    "\n",
    "1. They support GPU acceleration.\n",
    "2. They contain additional information (accessed as attributes, which we will see later) which facilitate the easy building and training of neural networks, via **computation graphs** and **gradients**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At9t1bdM7vp_"
   },
   "source": [
    "## A motivating example\n",
    "\n",
    "To see the advantages of PyTorch in action, let's first take a look at how one might train a simple model using only NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJo7qe3_77c_",
    "outputId": "acf732c8-36e5-4fe4-87d1-df0e29179d96"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create 2000 random input and output data points\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Step 0 - Initialization: Initialize model parameters (randomly, in this example)\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Step 1 - Forward Pass: compute predicted y according to the following model:\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Step 2 - Compute Loss (MSE in this example)\n",
    "    loss = np.square(y_pred - y).sum() # Sum across 2000 training points\n",
    "    if t % 100 == 0:\n",
    "        print(f'Epoch {t}: loss {loss}')\n",
    "\n",
    "    # Step 3 - Differentiate: Use backprop (chain rule) to compute gradients\n",
    "    # of the loss function with respect to a, b, c, d\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum() # Sum across 2000 training points\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Step 4 - Optimize: Update weights (vanilla GD in this example)\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')\n",
    "\n",
    "# From: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIfNG71OG9Jq"
   },
   "source": [
    "Each step involved in training a model is coded very explicitly in this example:\n",
    "0. Initialization: Initialize model parameters (i.e. weights)\n",
    "1. Forward Pass: Compute predictions\n",
    "2. Compute Loss\n",
    "3. Differentiate: Compute gradients via backpropagation (chain rule)\n",
    "4. Optimize: Update weights using gradients\n",
    "\n",
    "Notice how:\n",
    "- Every model parameter is stored as a variable, which contains a weight value to be continuously updated through training\n",
    "- The calculation of the gradient of the loss function with respect to the model parameters is coded explicitly and thus *model dependent*\n",
    "<!-- - The gradient of the loss function needs to be computed with respect to the model parameters explicitly using the chain rule -->\n",
    "- Each model parameter's gradient is used to update the weight value according to a particular update rule which is also explicitly implemented - in this example, standard gradient descent (GD).\n",
    "\n",
    "Steps 0 and 1 might not be so difficult using a basic package such as NumPy; however, one could imagine that steps 2 and 3 would quickly become too burdensome to implement manually for models with more parameters, both because of 1) the large number of parameters and 2) the fact that the explicit implementation of the chain rule and update rule will look different every time depending on the the model structure and optimization algorithm.\n",
    "\n",
    "This is where PyTorch's **automatic differentiation** engine, **Autograd**, is useful. As the name implies, PyTorch can take care of computing gradients and applying optimization update rules automatically under the hood, such that the code we write to invoke these two operaations will look (very nearly) the same across all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcRm_01MryEd"
   },
   "source": [
    "## Computation Graphs and Autograd\n",
    "\n",
    "<!-- Old paragraphs, commented out to preserve in case we want to bring back language -->\n",
    "<!-- Autograd, PyTorch's automatic differentiation engine, is one of the best things that PyTorch has to offer when it comes to training neural networks! Recall that in order to compute gradients for weights in a neural network, you have to backpropagate errors at each node. This is computationally intensive even for a one or two layers and would be a nightmare to implement manually for very deep networks. -->\n",
    "<!-- PyTorch tensors can keep track of a *computation graph* whenever operations are performed on/between tensors. Autograd performs backprop on this computation graph, so all the steps to compute the gradient are abstracted away. To perform a gradient step update, you simply have to call built-in PyTorch functions that computes the derivative of the loss and (if you're using gradient descent) takes a step toward a gradient of zero. -->\n",
    "\n",
    "To implement automatic differentiation, PyTorch keeps track of a **computation graph** which logs every operation used to construct a certain output or mathematical expression (more on computation graphs in the [Appendix](#Appendix)). Computation graphs are created automatically whenever mathematical operations are performed using tensors as inputs. Using this graph, **Autograd** can perform backpropagation by repeated applications of the chain rule, and all of these differentiation steps are abstracted away from the user. To perform weight updates, we simply call built-in PyTorch optimizer functions which implement different update rules using the gradients computed during backpropagation.\n",
    "\n",
    "As we work through the following example, keep in mind that computation graphs in PyTorch are *implicitly* constructed when one builds mathematical expressions. That is, one will never have to write code expressly for the purpose of building such a graph; rather, the computation graph will be assembled automatically using whichever Tensors or operations are appropriately \"earmarked\" for tracking (see `requires_grad` below).\n",
    "\n",
    "Let's start with a simple example of autograd. (Original example from https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-oQ3VCa-wEnL",
    "outputId": "7ec9b402-8075-4787-8813-6f3caa73d9c1"
   },
   "outputs": [],
   "source": [
    "# Create tensors\n",
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "print('x:', x)\n",
    "print('y:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoGuhefUwfz4"
   },
   "source": [
    "Tensors have a ```requires_grad``` field. When ```requires_grad == True``` for a certain tensor, any subsequent operations that this tensor is an input to will be incorporated into a computation graph. Autograd will then be able to automatically compute partial derivatives *with respect to this tensor*. (Derivative of what function? More on that [later](#step5).) \n",
    "\n",
    "For any tensors which are the outputs of operations tracked in the computation graph, the ```grad_fn``` attribute will indicate the operation that created the tensor, represented as a `Function` object. (It is these `Function`s which encode the computation graph DAG [Directed Acyclic Graph].)\n",
    "\n",
    "**Important: If some starting tensors have requires_grad = True, then every tensor resulting from operations of those starting tensors will also have requires_grad = True.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNy9_NsXwXPp",
    "outputId": "d858cd78-2ed9-46c5-c258-608f1182a675"
   },
   "outputs": [],
   "source": [
    "# By default, user created Tensors have ``requires_grad=False``\n",
    "print('x and y requires_grad:', x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "# So you can't backprop through z\n",
    "print('z grad_fn:', z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xnb3o2ayxFyX",
    "outputId": "96b31438-1758-45c7-c1b0-cdf619419b2c"
   },
   "outputs": [],
   "source": [
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad`` flag in-place.\n",
    "# The input flag defaults to ``True`` if not given.\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print('z grad_fn:', z.grad_fn)\n",
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print('z requires_grad:', z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjqGhfJ8yZJs"
   },
   "source": [
    "We can also detach a tensor from its computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0bYoOKBwKm0",
    "outputId": "f106444b-538b-40ff-fdcf-409d1c273da7"
   },
   "outputs": [],
   "source": [
    "# Now z has the computation history that relates itself to x and y\n",
    "# Can we just take its values, and **detach** it from its history?\n",
    "new_z = z.detach()\n",
    "\n",
    "# ... does new_z have information to backprop to x and y?\n",
    "# NO!\n",
    "print('new_z grad_fn:', new_z.grad_fn)\n",
    "# And how could it? ``z.detach()`` returns a tensor that shares the same storage as ``z``, but with the computation history forgotten.\n",
    "# It doesn't know anything about how it was computed.\n",
    "# In essence, we have broken the Tensor away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe2dUCuuTpo5"
   },
   "source": [
    "Now that we've been introduced to `requires_grad`, `grad_fn` and the computation graph....\n",
    "\n",
    "#### Question: Where should I actually set requires_grad to make everything work?\n",
    "#### Short Answer: In any tensors which represent model parameters or weights.\n",
    "\n",
    "Long Answer:\n",
    "\n",
    "- If you are initializing a parameter(s)/weight(s) by creating an individual tensor(s) manually, you have to set its `requires_grad = True` argument. \n",
    "- If you are initializing a parameter(s)/weight(s) by using built-in PyTorch methods from packages such as nn.Layer and nn.Parameter, the internally initialized tensors should already have `requires_grad` set to `True`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIFDbUwUrvCK"
   },
   "source": [
    "## GPU Acceleration\n",
    "\n",
    "GPUs are built for parallel processing — they can handle thousands of operations at the same time — which makes them great for computationally-intensive neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "7QdobpsapbDV"
   },
   "outputs": [],
   "source": [
    "# To run tensor operations on GPU in Colab, go to Edit -> Notebook settings -> and select GPU as the hardware accelerator.\n",
    "# If GPU is enabled, the following assertion should pass.\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djVoc6o0pqIn",
    "outputId": "2d3b6b38-2ca6-4e5f-b33d-0cf128936a67"
   },
   "outputs": [],
   "source": [
    "# When creating a tensor, set its device to use CUDA (a GPU parallel computing platform), as shown below.\n",
    "# What this means is that we are telling PyTorch to physically initialize the relevant \n",
    "# variable in GPU memory.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.rand(5, 5, device=device)\n",
    "y = torch.rand(5, 5, device=device)\n",
    "x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTYacvPiDex5"
   },
   "source": [
    "## Defining a Model\n",
    "\n",
    "* The ```nn``` package defines basic neural network layers (or Modules, in PyTorch-speak) as well as common loss functions. Modules take a tensor as input, apply some computation/function to it, and then output the new tensor. \n",
    "\n",
    "* The ```optim``` package defines common optimization algorithms such as Adam, RMSProp, and AdaGrad. These algorithms are all variations of stochastic (randomized) gradient descent, and they may differ in terms of how the gradient steps are taken and how the learning rate adapts over epochs. (More details here: https://pytorch.org/docs/stable/optim.html.)\n",
    "\n",
    "### Example: Cubic Model\n",
    "Let's say we want to predict y's modeled as ```ax + bx^2 + cx^3```, and our goal is to learn the weights ```a```, ```b```, and ```c```. (Original example from https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#id21.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms74sAOfKw1x"
   },
   "source": [
    "### Step 1: Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nb9Qvif0KOmh",
    "outputId": "33fd2a33-beac-4bb6-cfdb-ecf0e792feed"
   },
   "outputs": [],
   "source": [
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Prepare the input tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p) \n",
    "print(xx.shape)\n",
    "# Unsqueeze inserts a dimension of size one at the specified location; the tensor\n",
    "# is reshaped to (2000, 1). Since p is a tensor, pow is applied according to\n",
    "# broadcasting rules; xx has a final shape of (2000, 3). See documentation for \n",
    "# more on broadcasting; PyTorch supports NumPy broadcasting semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZrswir2K0fp"
   },
   "source": [
    "<a name=\"step2\"></a>\n",
    "\n",
    "### Step 2: Define Model (using nn package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "soqgPNLtK5mG"
   },
   "outputs": [],
   "source": [
    "# Use the nn package to define our model as a sequence of layers.\n",
    "# nn.Sequential is a Module which contains other Modules, and applies them in sequence to produce its output.\n",
    "# nn.Linear computes the 3 input features (x, x^2, and x^3) into 1 output feature using a linear function,\n",
    "# holding internal tensors for its weight and bias.\n",
    "# nn.Flatten flattens the output of the linear layer to a 1D tensor to match the shape of y.\n",
    "# In this case, the first dim it flattens is at index 0, and the last dim it flattens is at index 1.\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE6UFoGcMrbE"
   },
   "source": [
    "### Step 3: Define Loss Function (using nn package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "AwHrUXgPMqfY"
   },
   "outputs": [],
   "source": [
    "# In this case, we will use Mean Squared Error. \n",
    "# You can either implement loss functions manually, or\n",
    "# you can use PyTorch's implementations of commonly-used loss functions.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAEuLYyAM1UD"
   },
   "source": [
    "### Step 4: Define Optimizer (using optim package)\n",
    "\n",
    "Use the optim package to define an Optimizer that will update the weights of the model for us. Here we will use RMSprop; the optim package contains many other optimization algorithms. The first argument to the RMSprop constructor tells the optimizer which Tensors it should update.\n",
    "\n",
    "In general, what distinguishes different optimizers is the way that they use gradients to define an update rule or scheme for their model weights. It follows then that any optimizer will always need the weights to be updated, and their corresponding gradients. In PyTorch, both of these quantities are stored together under model parameter variables. In particular, if `var` is a model weight, `var.grad` will contain a gradient with respect to `var` after backpropagation is performed. More in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MdSCSU5nM4bG"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwyFsv2nM-5K"
   },
   "source": [
    "<a name=\"step5\"></a>\n",
    "\n",
    "### Step 5: Train the Model\n",
    "\n",
    "Every time you train a model, be sure to include these 3 short but important lines that do much of the heavy lifting for you!\n",
    "\n",
    "1.   `optimizer.zero_grad()`: Zeroes out gradients from previous iterations. Otherwise, over the epochs, gradients will accumulate (sum) in `parameter.grad` of each `parameter` which was passed to the optimizer as an argument.\n",
    "2.   `loss.backward()`: Does all the backpropagation in one fell swoop (takes the derivative of the loss). We call `.backward()` on the loss tensor, which has the effect of taking the partial derivative of the loss with respect to the model parameters. For a model parameter (weight) `w`, dloss/dw will be stored in `w.grad` after `loss.backward()`.\n",
    "3. `optimizer.step()`: Based on the previously specified optimizer algorithm and learning rate, updates the parameters in the direction of the gradient. This represents one iteration of the weight update rule.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IOOcYAkcNAgu",
    "outputId": "8abc5209-3305-473e-e8c3-2d6ce8a08ee0"
   },
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if epoch % 100 == 99:\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "    # Before the backward pass, zero all the gradients for the variables it will update\n",
    "    # (these are the learnable weights of the model). This is because by defaults, \n",
    "    # gradients from previous iterations won't be overwritten. \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # After taking the backward pass, you can see the computed gradient for each parameter\n",
    "    if epoch % 100 == 99:\n",
    "      # The model implicitly stores the parameters, as shown below:\n",
    "      for i, param in enumerate(model.parameters()):\n",
    "        # param.grad refers to d_loss/d_parameter\n",
    "        print('parameter', i, 'gradient', param.grad)\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    optimizer.step()\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3uuSWYTSLvz"
   },
   "source": [
    "# **Problem 3, Part 1: PyTorch Concept Questions**\n",
    "\n",
    "*Write up your answers to these problems in your PDF submission.  Any work included in your Colab notebook will not be graded.*\n",
    "\n",
    "These questions are intended to be a straightforward check on some fundamental PyTorch concepts. You are encouraged to use the PyTorch documentation and any other online resources to answer these questions.  Please write no more than 3 sentences to answer each of the below questions.\n",
    "\n",
    "# Problem 3.1\n",
    "Where are gradients stored in PyTorch and how can they be accessed?\n",
    "# Problem 3.2\n",
    "In the model defined in [Step 2](#step2) in the tutorial section, how many weights does the model have, and which layer do they come from?\n",
    "# Problem 3.3\n",
    "Conceptually, what is a fully-connected layer in a neural network, and how is it represented in PyTorch?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jt4jB6bKNcH"
   },
   "source": [
    "# **Problem 3, Part 2: Training a Neural Network on CIFAR-10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOftgxoIsiZl"
   },
   "source": [
    "In this part, you will train two neural networks to perform multi-class classifications on the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). This dataset consists of 32x32 images belonging to 10 classes. In particular:\n",
    "\n",
    "\n",
    "\n",
    "# Problem 3.4\n",
    "**Initialize:** Complete the Part2NeuralNetwork class with 3 fully connected (linear) layers using ReLU activation functions, and with the hidden layers having 1000 nodes each.\n",
    "\n",
    "Here, we ask you to define a model class which inherits from `nn.Module` rather than using `nn.Sequential` as we saw before in [Step 2](#step2). This is done by initializing weights in `__init__()` and manually implementing the forward pass in `forward()`. Feel free to reference documentation.\n",
    "\n",
    "Please copy and paste your neural network class into your PDF writeup.  We recommend using a LaTeX library such as [listings](https://ctan.org/pkg/listings?lang=en) to ensure your code renders correctly.\n",
    "\n",
    "*Include in your submission: Your neural network class code.*\n",
    "\n",
    "# Problem 3.5\n",
    "\n",
    "**Train:** Complete the training code for the model. Use [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) and an optimizer of your choice.\n",
    "\n",
    "Train your model for 10 epochs. Every epoch, log the train and test set loss.  Create a plot that displays both the model's train and the test set loss across epochs.\n",
    "\n",
    "*Include in your submission: 1 plot which has both the train and test loss vs. number of epochs.*\n",
    "\n",
    "# Problem 3.6\n",
    "\n",
    "**Evaluate:** To evaluate your model, compute the following metrics on the train and test set, including them in your write-up.  For each of the following metrics, use the model's \"hard assignment\" of labels, i.e. your model's predictions should assign each data point to only 1 class:\n",
    "\n",
    "* The model's classification accuracy\n",
    "* The model's [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) for each of the 10 classes\n",
    "\n",
    "What kinds of errors do you believe that the trained model is making?  Use evidence (such as the above metrics, example misclassified images, or any other metrics of your choice) to support your claims.\n",
    "\n",
    "*Include in your submission: train/test accuracy, train/test precision/recall for 10 classes, and evidence to support your hypothesis about the model's errors.*\n",
    "\n",
    "# Problem 3.7\n",
    "\n",
    "**Explore:** Create a new neural network with at least 1 architectural modification.   Some things you can explore are adding or removing layers, changing the number of nodes at each layer, or experimenting with convolutional and pooling layers (see Appendix). The only requirement is that your model attain at least 50% test set accuracy after training for 10 epochs.  This part of the problem is intentionally open-ended, and we encourage you to explore!\n",
    "\n",
    "For your new neural network, include a plot of train and test set loss in your writeup.  Calculate your model's train/test accuracy and precision/recall for the 10 classes.\n",
    "\n",
    "In your writeup, copy and paste your modified neural network class and describe the architectural changes you made.  Write at least 1 sentence about why you hypothesize your new neural network performed better or performed worse than the network in Part 3.4.\n",
    "\n",
    "*Include in your submission: Your neural network class code, 1 plot of train/test loss, metrics from Part 3.6 for this new neural net, and explanation of performance.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ND3B7K0TKPyJ"
   },
   "outputs": [],
   "source": [
    "## Download CIFAR-10 dataset \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./part_2_data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./part_2_data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "c-t27AQYtPHc"
   },
   "outputs": [],
   "source": [
    "# TODO - Complete Part2NeuralNetwork. Include class into your PDF submission!\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Part2NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Part2NeuralNetwork, self).__init__()\n",
    "        ## TODO: Define your neural network layers here!\n",
    "        ## Importantly, any modules which initialize weights should be initialized\n",
    "        ## as member variables here. \n",
    "        ## Note: Keep track of the shape of your input tensors in the training\n",
    "        ## and test sets, because it affects how you define your layers!\n",
    "        ## You might find this resource helpful:\n",
    "        ## https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## TODO: This is where you should apply the layers defined in the __init__\n",
    "        ## method and the ReLU activation functions to the input x.\n",
    "        return x\n",
    "\n",
    "# Display model architecture\n",
    "model = Part2NeuralNetwork()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "zfaC1lDuek_i"
   },
   "outputs": [],
   "source": [
    "# TODO - Complete NN training\n",
    "\n",
    "# Reinitialize to ensure we are training a new model\n",
    "model = Part2NeuralNetwork()\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    model.train(True)\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs; data is a list of [inputs, labels]\n",
    "        xs, ys = data\n",
    "        xs = xs.to(device)\n",
    "        ys = ys.to(device)\n",
    "\n",
    "        ## TODO: optimize model parameters\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    train_losses.append(running_loss / 50000)\n",
    "    print('[Epoch %d] average train loss: %.3f' % (epoch + 1, running_loss / 50000))\n",
    "    \n",
    "    model.train(False)\n",
    "    running_test_loss = 0.0\n",
    "    ## TODO: calculate test loss in similar fashion\n",
    "\n",
    "    test_losses.append(running_test_loss / 10000)\n",
    "    print('[Epoch %d] average test loss: %.3f' % (epoch + 1, running_test_loss / 10000))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "6eM9cwr8Jrfc"
   },
   "outputs": [],
   "source": [
    "# Use this function to compute your model accuracy!\n",
    "# Note that the dataloader argument can take trainloader or testloader\n",
    "def compute_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train(False)\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            xs, ys = data\n",
    "            xs = xs.to(device)\n",
    "            ys = ys.to(device)\n",
    "            y_preds = model(xs)\n",
    "            _, predicted = torch.max(y_preds.data, 1)\n",
    "            total += ys.size(0)\n",
    "            correct += (predicted == ys).sum().item()\n",
    "    print('Model accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHkX9Z6q27-4"
   },
   "source": [
    "<a name=\"Appendix\"></a>\n",
    "# Appendix: Computation Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaUHTpkpUOHn"
   },
   "source": [
    "A *computation graph* is an abstract graphical representation of a mathematical expression involving variables. Each variable is represented as an edge, and each operation as a node; the graph is directed and acyclic. For instance, the expression $(x + y)^2 + x$ can be represented as:\n",
    "\n",
    "![Computation Graph](https://github.com/harvard-ml-courses/cs181-s21-homeworks/blob/main/hw3/T3_P3_computation_graph.jpg?raw=true)\n",
    "\n",
    "Expressions can be converted into graphs and vice versa. Computation graphs are useful representations in the context of neural networks, because\n",
    "1. They encode all the necessary information for the computation of gradients into discrete units through repeated application of the chain rule, and\n",
    "2. Graphs are easy to implement with code (conceptually, one can imagine storing pointers between nodes)\n",
    "\n",
    "The process in step #1 is known as *automatic differentiation*.\n",
    "\n",
    "If you would like to walk through a computation graph on a small neural network, we recommend [this resource](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztTJRpX09z6J"
   },
   "source": [
    "# Appendix: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fNNaZso91zx"
   },
   "source": [
    "A convolutional neural network (CNN) is a special type of neural network that has \"convolution layers\". CNNs are able to model spatial and temporal dependencies in ordered sequences of input data.\n",
    "\n",
    "The convolution layer of a CNN applies a filter (sometimes referred to as a \"kernel\") to an input to produce an output. \n",
    "\n",
    "Assume without loss of generality that we have two-dimensional input data $x : m \\times n$.  For example, $x$ may be the color of each pixel of a grayscale $m$ by $n$ image.\n",
    "\n",
    "During the forward pass, we slide (or \"convolve\") the filter across the input data.  In most CNNs, the filter is a function of \"regions\" of the data, where individual components of the input are grouped together.  For example, a convolutional filter applied to image data may take into account $3 x 3$ groups of pixels to model dependencies in between adjacent pixels:\n",
    "\n",
    "![CNN sliding window](https://courses.edx.org/assets/courseware/v1/7396a332c596cb606eae215d96301334/asset-v1:MITx+6.871Jx+2T2021+type@asset+block/cnn_sliding_window.gif)\n",
    "\n",
    "This GIF image shows a sliding convolutional 3 x 3 filter, being applied to 3 x 3 segments of a 4 x 4 dataset.\n",
    "\n",
    "source: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n",
    "\n",
    "Another example of a 1-dimensional convolution filter is illustrated by the below GIF:\n",
    "\n",
    "![Convolution operation for a simple (1 x 3) filter](https://studio.edx.org/assets/courseware/v1/20da14e015eac1863451dafe3d456e59/asset-v1:MITx+6.871Jx+2T2021+type@asset+block/cnn.gif)\n",
    "\n",
    "This animation is described below.\n",
    "\n",
    "In this example, the filter $[2, 3, 1]$ is applied as an inner product with the input vector $x = [x_1, x_2, ..., x_{10}]: 1 \\times 10$ which contains annotations of a patient EKG.  Notice here that the filter is applied to 3 successive time-steps of the input vector: the output $y_t$ at time $t$ is equal to $[2, 3, 1] \\cdot [x_t, x_{t + 1}, t_{t + 2}]$.  Therefore the output $y_t$ contains information not only about a single EKG annotation $x_t$, but also about nearby EKG annotations $x_{t + 1}$ and $x_{t + 2}$.\n",
    "\n",
    "We strongly recommend [reading this article](https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/) which walks through how to create a CNN using conv and pooling layers in PyTorch.\n",
    "\n",
    "If you are interested, you can also watch this [MIT lecture](https://www.youtube.com/watch?v=iaSUYvmCekI) on CNNs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
